{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6bfe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d64d8c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of CUDA device:      NVIDIA GeForce GT 740M\n"
     ]
    }
   ],
   "source": [
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"Name of CUDA device:\\\n",
    "      {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "97e3b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "54bbdf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "7451242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2593\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>IC50</th>\n",
       "      <th>units</th>\n",
       "      <th>activity</th>\n",
       "      <th>pIC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL477</td>\n",
       "      <td>Nc1ncnc2c1ncn2[C@@H]1O[C@H](CO)[C@@H](O)[C@H]1O</td>\n",
       "      <td>1600000.00</td>\n",
       "      <td>nM</td>\n",
       "      <td>low</td>\n",
       "      <td>15.204120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL433520</td>\n",
       "      <td>CCN(CC)CC(O)CNc1cc2c(Nc3cccc(Br)c3)ncnc2cn1</td>\n",
       "      <td>1100000.00</td>\n",
       "      <td>nM</td>\n",
       "      <td>low</td>\n",
       "      <td>15.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL440298</td>\n",
       "      <td>N#C/C(=C\\c1ccc(O)c(O)c1)C(=O)NCCCCc1ccccc1</td>\n",
       "      <td>501187.23</td>\n",
       "      <td>nM</td>\n",
       "      <td>low</td>\n",
       "      <td>14.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL162034</td>\n",
       "      <td>CN(C)CCNc1cc2c(Nc3cccc(Br)c3)ncnc2cn1</td>\n",
       "      <td>379000.00</td>\n",
       "      <td>nM</td>\n",
       "      <td>low</td>\n",
       "      <td>14.578639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL56319</td>\n",
       "      <td>CCOC(=O)C(Cc1ccccc1)NC(=O)/C(C#N)=C/c1ccc(O)c(...</td>\n",
       "      <td>331131.12</td>\n",
       "      <td>nM</td>\n",
       "      <td>low</td>\n",
       "      <td>14.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                                             smiles  \\\n",
       "0     CHEMBL477    Nc1ncnc2c1ncn2[C@@H]1O[C@H](CO)[C@@H](O)[C@H]1O   \n",
       "1  CHEMBL433520        CCN(CC)CC(O)CNc1cc2c(Nc3cccc(Br)c3)ncnc2cn1   \n",
       "2  CHEMBL440298         N#C/C(=C\\c1ccc(O)c(O)c1)C(=O)NCCCCc1ccccc1   \n",
       "3  CHEMBL162034              CN(C)CCNc1cc2c(Nc3cccc(Br)c3)ncnc2cn1   \n",
       "4   CHEMBL56319  CCOC(=O)C(Cc1ccccc1)NC(=O)/C(C#N)=C/c1ccc(O)c(...   \n",
       "\n",
       "         IC50 units activity      pIC50  \n",
       "0  1600000.00    nM      low  15.204120  \n",
       "1  1100000.00    nM      low  15.041393  \n",
       "2   501187.23    nM      low  14.700000  \n",
       "3   379000.00    nM      low  14.578639  \n",
       "4   331131.12    nM      low  14.520000  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/her_molecules.csv\")\n",
    "print(f\"size: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f96f10a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['low', 'medium', 'high'], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "620e666a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       1097\n",
       "high      1048\n",
       "medium     448\n",
       "Name: activity, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ba5bfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c1e48abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = CustomDataset(df.smiles, df.activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "faa34668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2074.4], [259.3], [259.3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "[0.8 *len(dataset)], [0.1 *len(dataset)], [0.1 *len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "30d62ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2593"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [2075, 259, 259]) # 80, 10, 10\n",
    "len(train_dataset + test_dataset + val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fcf5f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID:\n",
      "  CO/N=C/c1c(N)ncnc1Nc1cccc(Br)c1\n",
      "Label:\n",
      " medium\n"
     ]
    }
   ],
   "source": [
    "for smiles, labels in train_dataset:\n",
    "    print(\"Input ID:\\n \" ,smiles)\n",
    "    print(\"Label:\\n\" ,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "37109c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/topazape/LSTM_Chem/blob/master/lstm_chem/utils/smiles_tokenizer2.py\n",
    "\n",
    "class SmilesTokenizer(object):\n",
    "    def __init__(self):\n",
    "        atoms = [\n",
    "            'Al', 'As', 'B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'K', 'Li', 'N',\n",
    "            'Na', 'O', 'P', 'S', 'Se', 'Si', 'Te'\n",
    "        ]\n",
    "        special = [\n",
    "            '(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5',\n",
    "            '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
    "        ]\n",
    "\n",
    "        self.table = sorted(atoms, key=len, reverse=True) + special \n",
    "\n",
    "        self.table_2_chars = list(filter(lambda x: len(x) == 2, self.table))\n",
    "        self.table_1_chars = list(filter(lambda x: len(x) == 1, self.table))\n",
    "        self.vocab_dict = {}\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        smiles = smiles + ' '\n",
    "        N = len(smiles)\n",
    "        token = []\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            c1 = smiles[i]\n",
    "            c2 = smiles[i:i + 2]\n",
    "\n",
    "            if c2 in self.table_2_chars:\n",
    "                token.append(c2)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            if c1 in self.table_1_chars:\n",
    "                token.append(c1)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return np.asarray(token, dtype=object)\n",
    "        \n",
    "    def vocaburaly(self):\n",
    "        vocab_dict = {}\n",
    "        for i, tok in enumerate(self.table):\n",
    "            vocab_dict[tok] = i\n",
    "        return vocab_dict\n",
    "    \n",
    "    def index_encode(self, tokenized_smiles):\n",
    "        vocab_dict = {}\n",
    "        for i, tok in enumerate(self.table):\n",
    "            vocab_dict[tok] = i\n",
    "        encoded = [vocab_dict[t] for t in tokenized_smiles ]\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4632b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SmilesTokenizer()\n",
    "tokens = [tokenizer.tokenize(x) for x in df.smiles]\n",
    "vocabulary = tokenizer.vocaburaly()\n",
    "indexed_smiles = [tokenizer.index_encode(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "48e9ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab1 = vocab(ordered_dict)\n",
    "vocab1.insert_token(\"<pad>\", 0)\n",
    "vocab1.insert_token(\"<unk>\", 1)\n",
    "vocab1.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "95ed6b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 43), ('o', 42), ('n', 41), ('c', 40), ('te', 39), ('se', 38), ('-', 37)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_freq_tuples[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2334133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', 's', 'o', 'n', 'c', 'te', 'se', '-', '+']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "59bd161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocab1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f7d691b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (text, label) in train_dataset]))\n",
    "print(f\"Number of classes: {num_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c80a36c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 22, 27, 26, 22, 29, 25]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: [vocab1[token] for token in tokenizer.tokenize(x)]\n",
    "text_pipeline(\"O=S(=O)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "28c4e777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline = lambda x: 0 if x == \"low\" else (1 if x == \"medium\" else 2)\n",
    "label_pipeline(\"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "074984fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for   _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text),\n",
    "                                      dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list,\n",
    "                                 dtype=torch.long)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
    "                                                     batch_first=True)  \n",
    "    return padded_text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "24a8018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35, 22, 35,  ...,  0,  0,  0],\n",
       "         [35,  5, 18,  ...,  0,  0,  0],\n",
       "         [35, 35, 29,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [30, 21, 35,  ...,  0,  0,  0],\n",
       "         [ 5, 18,  5,  ...,  0,  0,  0],\n",
       "         [35, 29,  5,  ...,  0,  0,  0]]),\n",
       " tensor([2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2,\n",
       "         0, 0, 2, 0, 1, 0, 0, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2,\n",
       "         2, 0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 2,\n",
       "         0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0,\n",
       "         0, 2, 0, 2, 1, 0, 2, 0, 2, 2, 0, 1, 1, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1,\n",
       "         0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "         0, 0, 0, 1, 0, 2, 0, 2, 1, 1, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2,\n",
       "         2, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 2,\n",
       "         2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0,\n",
       "         1, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0,\n",
       "         0, 0, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 1, 2, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b449784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=20, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=20, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "306151be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35, 24, 35, 33, 23, 26, 30,  5, 18,  4,  5,  4,  5, 17, 24,  4, 33, 23,\n",
      "          5, 26,  8,  5, 16,  5,  5,  5, 26, 30, 35, 26, 22, 29, 25, 35, 35, 35,\n",
      "         35, 35, 35, 35, 26, 22, 29, 25, 30, 29, 25,  5,  5, 16, 25,  5,  5, 18,\n",
      "         17, 25,  5, 18,  5,  5,  5,  5,  5, 18,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [29, 22, 35, 26, 30,  5, 18,  5,  5,  5, 26, 43, 25,  5,  5, 18, 25,  5,\n",
      "         18,  5,  5, 26, 43, 25,  5,  5, 26, 43, 25,  5, 18, 29,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2])\n",
      "torch.Size([20, 103])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(test_dataloader))\n",
    "print(text_batch[:2])\n",
    "print(label_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "968a32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # in_channels; num of channels, we use 1 since its text only\n",
    "        # out_channels; output feature map\n",
    "        # kernel_size; the size of filters i.e. [n-grams size x emb_dim]\n",
    "        # if 3 filters are provided, we get 3 convolution layers\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (filt, embedding_dim)) \n",
    "                                    for filt in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # if we have 3 filter sizes, we get e linear layers\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        # text = [batch size, sent len]\n",
    "    def forward(self, text):\n",
    "        \n",
    "        # rearange the original tensor to 1 dimensional tensor\n",
    "        # text dim ==> # text = [batch size, sent len]\n",
    "#         text = text.permute(1,0)\n",
    "        \n",
    "        # add embedding dimension to text\n",
    "        # [batch size, sent len] ==> [batch size, sent len, emb dim]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # add 1 dimensional \n",
    "        # [batch size, sent len, emb dim]==> [batch size, 1, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        # application of Relu activation function\n",
    "        # foward pass of embedd text to convolution layers\n",
    "        # squeeze() to drop superficial 3 dimensional from a tensor\n",
    "        # conved dim ==> [batch size, n_filters, sent len * len(filter_sizes)]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        # max pooling to decrease num of features (sub-sampling)\n",
    "        # squeeze() to drop superficial 2 dimensional from a tensor\n",
    "        # pooled dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # concatenate feature outputs\n",
    "        # cat dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        cat = torch.cat(pooled, dim = 1)\n",
    "        \n",
    "        # dropout on concatenated output filters\n",
    "        # out dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        out = self.dropout(cat)\n",
    "        \n",
    "        # pass filter outputs to linear layer for prediction\n",
    "        # pred dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        pred = self.fc(out)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0a664",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Here, I'll instantiate the network. First up, defining the hyperparameters.\n",
    "    vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "    output_size: Size of our desired output; the number of class scores we want to output.\n",
    "    embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "    num_filters: Number of filters that each convolutional layer produces as output.\n",
    "    filter_sizes: A list of kernel sizes; one convolutional layer will be created for each kernel size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "36b14f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 60 ## len of vocab size\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = len(df.activity.unique()) ## num_classes 3\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "91feee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(60, 100, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f744f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    # model training mode (gradient computation)\n",
    "    model.train()\n",
    "    # initailiz acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    for idx, (text, label) in enumerate (dataloader):\n",
    "        text = text.to(device)\n",
    "        label = label.to(device)\n",
    "        # reset gradients to zero before each instance\n",
    "        optimizer.zero_grad()\n",
    "        # label predictions (forward papagation)\n",
    "        # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "        predicted_label = model(text).squeeze(1) # or [:,0]\n",
    "        # loss calculation\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        # compute gradients (backward propagation) \n",
    "        # to minimize loss functions with gradient descent\n",
    "        loss.backward()\n",
    "        # update parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "        # logging\n",
    "        if not idx % 50:\n",
    "            print(f\"Epoch: {epoch + 1:04d}/{num_epochs:0d} | \"\n",
    "                  f\"Batch {idx:03d}/{len(dataloader):03d} | \"\n",
    "                  f\"Loss: {loss:.4f}\")\n",
    "        # compute total accuracy\n",
    "        # return an indice of the max value of all elements\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        # compute total loss after back prop and parameter update\n",
    "        total_loss += loss.item() * label.size(0)\n",
    "    # compare true labels with the predicted labels to compute accuracy\n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "            total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f5b6545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    # model evaluation mode (no gradient computation)\n",
    "    model.eval()\n",
    "    # initailize acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    # disabling gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for text, label in dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            # label predictions (forward papagation)\n",
    "            # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "            predicted_label = model(text).squeeze(1) # reshape\n",
    "            # loss calculation\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            # compute total accuracy\n",
    "            # return an indice of the max value of all elements\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            # compute total loss after back prop and parameter update\n",
    "            total_loss += loss.item() * label.size(0)\n",
    "        # compare true labels with the predicted labels to compute accuracy\n",
    "        return total_acc/len(dataloader.dataset), \\\n",
    "                total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8ef067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiclass classification we use\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Adam Optimizer to update parameters based on the computed gradients\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c5ed3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c632c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/20 | Batch 000/104 | Loss: 0.3934\n",
      "Epoch: 0001/20 | Batch 050/104 | Loss: 0.4900\n",
      "Epoch: 0001/20 | Batch 100/104 | Loss: 0.6955\n",
      "Train Acc.: 72.53%\n",
      "Valid Acc.: 73.36%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 0002/20 | Batch 000/104 | Loss: 0.5976\n",
      "Epoch: 0002/20 | Batch 050/104 | Loss: 0.4894\n",
      "Epoch: 0002/20 | Batch 100/104 | Loss: 0.7770\n",
      "Train Acc.: 74.22%\n",
      "Valid Acc.: 73.36%\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 0003/20 | Batch 000/104 | Loss: 0.6237\n",
      "Epoch: 0003/20 | Batch 050/104 | Loss: 0.7085\n",
      "Epoch: 0003/20 | Batch 100/104 | Loss: 0.4727\n",
      "Train Acc.: 73.49%\n",
      "Valid Acc.: 69.11%\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 0004/20 | Batch 000/104 | Loss: 0.5131\n",
      "Epoch: 0004/20 | Batch 050/104 | Loss: 0.6451\n",
      "Epoch: 0004/20 | Batch 100/104 | Loss: 0.6127\n",
      "Train Acc.: 74.46%\n",
      "Valid Acc.: 71.04%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 0005/20 | Batch 000/104 | Loss: 0.4203\n",
      "Epoch: 0005/20 | Batch 050/104 | Loss: 0.2641\n",
      "Epoch: 0005/20 | Batch 100/104 | Loss: 0.5239\n",
      "Train Acc.: 74.07%\n",
      "Valid Acc.: 68.73%\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 0006/20 | Batch 000/104 | Loss: 0.5234\n",
      "Epoch: 0006/20 | Batch 050/104 | Loss: 0.7632\n",
      "Epoch: 0006/20 | Batch 100/104 | Loss: 0.4126\n",
      "Train Acc.: 76.24%\n",
      "Valid Acc.: 76.06%\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 0007/20 | Batch 000/104 | Loss: 0.5079\n",
      "Epoch: 0007/20 | Batch 050/104 | Loss: 0.4256\n",
      "Epoch: 0007/20 | Batch 100/104 | Loss: 0.6392\n",
      "Train Acc.: 74.51%\n",
      "Valid Acc.: 76.45%\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 0008/20 | Batch 000/104 | Loss: 0.6412\n",
      "Epoch: 0008/20 | Batch 050/104 | Loss: 0.7368\n",
      "Epoch: 0008/20 | Batch 100/104 | Loss: 0.3953\n",
      "Train Acc.: 75.28%\n",
      "Valid Acc.: 76.45%\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 0009/20 | Batch 000/104 | Loss: 0.4467\n",
      "Epoch: 0009/20 | Batch 050/104 | Loss: 0.4642\n",
      "Epoch: 0009/20 | Batch 100/104 | Loss: 0.7023\n",
      "Train Acc.: 75.71%\n",
      "Valid Acc.: 74.90%\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 0010/20 | Batch 000/104 | Loss: 0.5288\n",
      "Epoch: 0010/20 | Batch 050/104 | Loss: 0.5809\n",
      "Epoch: 0010/20 | Batch 100/104 | Loss: 0.4364\n",
      "Train Acc.: 76.14%\n",
      "Valid Acc.: 76.83%\n",
      "Time elapsed: 0.69 min\n",
      "Epoch: 0011/20 | Batch 000/104 | Loss: 0.6451\n",
      "Epoch: 0011/20 | Batch 050/104 | Loss: 0.3896\n",
      "Epoch: 0011/20 | Batch 100/104 | Loss: 0.6636\n",
      "Train Acc.: 75.71%\n",
      "Valid Acc.: 72.59%\n",
      "Time elapsed: 0.75 min\n",
      "Epoch: 0012/20 | Batch 000/104 | Loss: 0.5754\n",
      "Epoch: 0012/20 | Batch 050/104 | Loss: 0.7762\n",
      "Epoch: 0012/20 | Batch 100/104 | Loss: 0.3949\n",
      "Train Acc.: 76.72%\n",
      "Valid Acc.: 76.45%\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 0013/20 | Batch 000/104 | Loss: 0.4292\n",
      "Epoch: 0013/20 | Batch 050/104 | Loss: 0.4504\n",
      "Epoch: 0013/20 | Batch 100/104 | Loss: 0.4273\n",
      "Train Acc.: 77.64%\n",
      "Valid Acc.: 73.75%\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 0014/20 | Batch 000/104 | Loss: 0.3043\n",
      "Epoch: 0014/20 | Batch 050/104 | Loss: 0.4152\n",
      "Epoch: 0014/20 | Batch 100/104 | Loss: 0.5283\n",
      "Train Acc.: 77.54%\n",
      "Valid Acc.: 76.06%\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 0015/20 | Batch 000/104 | Loss: 0.2293\n",
      "Epoch: 0015/20 | Batch 050/104 | Loss: 0.5256\n",
      "Epoch: 0015/20 | Batch 100/104 | Loss: 0.3913\n",
      "Train Acc.: 77.69%\n",
      "Valid Acc.: 76.83%\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 0016/20 | Batch 000/104 | Loss: 0.7816\n",
      "Epoch: 0016/20 | Batch 050/104 | Loss: 0.7207\n",
      "Epoch: 0016/20 | Batch 100/104 | Loss: 0.5710\n",
      "Train Acc.: 78.55%\n",
      "Valid Acc.: 75.29%\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 0017/20 | Batch 000/104 | Loss: 0.6581\n",
      "Epoch: 0017/20 | Batch 050/104 | Loss: 0.6073\n",
      "Epoch: 0017/20 | Batch 100/104 | Loss: 0.3427\n",
      "Train Acc.: 78.55%\n",
      "Valid Acc.: 75.68%\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 0018/20 | Batch 000/104 | Loss: 0.8464\n",
      "Epoch: 0018/20 | Batch 050/104 | Loss: 0.3127\n",
      "Epoch: 0018/20 | Batch 100/104 | Loss: 0.5374\n",
      "Train Acc.: 77.88%\n",
      "Valid Acc.: 76.06%\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 0019/20 | Batch 000/104 | Loss: 0.4107\n",
      "Epoch: 0019/20 | Batch 050/104 | Loss: 0.0719\n",
      "Epoch: 0019/20 | Batch 100/104 | Loss: 0.6551\n",
      "Train Acc.: 79.61%\n",
      "Valid Acc.: 76.06%\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 0020/20 | Batch 000/104 | Loss: 0.6496\n",
      "Epoch: 0020/20 | Batch 050/104 | Loss: 0.6956\n",
      "Epoch: 0020/20 | Batch 100/104 | Loss: 0.5045\n",
      "Train Acc.: 78.84%\n",
      "Valid Acc.: 77.99%\n",
      "Time elapsed: 1.33 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dataloader)\n",
    "    acc_val , loss_val = evaluate(val_dataloader)\n",
    "    print(f\"Train Acc.: {100 * acc_train:.2f}%\"\n",
    "          f\"\\nValid Acc.: {100 * acc_val:.2f}%\")\n",
    "    print(f'Time elapsed: {(time.time() - start_time) / 60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcfec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8d524686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc.: 76.06%\n"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dataloader)\n",
    "print(f\"Test Acc.: {100 * acc_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9600260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1,\n",
    "# we use the argmax to get the highest predicted class index. \n",
    "# or we use sofmax activation function and squash the input to range 0 and 1 and sum them to 1\n",
    "# in this case we get both the class label pred and the predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "b7d29ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2_lstm.ipynb\n",
    "\n",
    "sentiment_label = {0: \"low\",\n",
    "                   1: \"medium\",\n",
    "                   2: \"high\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    # evaluation mode (no gradients)\n",
    "    with torch.no_grad():\n",
    "        # tokenize and index the tokens\n",
    "        processed_text = torch.tensor(text_pipeline(text))\n",
    "        # add a batch dimension\n",
    "        processed_text = processed_text.unsqueeze(0).to(device)\n",
    "        # prediction\n",
    "        prediction = model(processed_text) # logits\n",
    "        # reduction real numbers to values between 0 and 1 and sum to 1\n",
    "        probability = torch.softmax(prediction, dim=1)\n",
    "        # get the max value of all elements in the input tensor\n",
    "        predicted_probability, predicted_class, = torch.max(probability, dim=1)\n",
    "        # convert tensor holding a single value into an integer\n",
    "        return predicted_class.item(), predicted_probability.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "b042e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0 = low\n",
      "Probability: 0.8340631723403931\n"
     ]
    }
   ],
   "source": [
    "text = \"Nc1ncnc2c1ncn2[C@@H]1O[C@H](CO)[C@@H](O)[C@H]1O\"\n",
    "pred_class, pred_proba = predict(text, text_pipeline)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "32a20e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2 = high\n",
      "Probability: 0.7394805550575256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"O=C(C#CCCN1CCOCC1)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncn\"\n",
    "pred_class, pred_proba = predict(text, text_pipeline)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
