{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0148fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618b79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7451242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "      <th>HIV_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>COC(C(=O)C(O)C(C)O)C1Cc2cc3cc(OC4CC(OC5CC(O)C(...</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>N=c1[nH][nH]c(=N)[nH]1.O=C(O)C(=O)O</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>CC12COP(OC1)OC2</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>C1C2CC3CC1OP(O2)O3</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>CC1(c2ccc(Cl)cc2)OCC(CCl)O1</td>\n",
       "      <td>CI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles activity  HIV_active\n",
       "1995  COC(C(=O)C(O)C(C)O)C1Cc2cc3cc(OC4CC(OC5CC(O)C(...       CI           0\n",
       "1996                N=c1[nH][nH]c(=N)[nH]1.O=C(O)C(=O)O       CI           0\n",
       "1997                                    CC12COP(OC1)OC2       CI           0\n",
       "1998                                 C1C2CC3CC1OP(O2)O3       CI           0\n",
       "1999                        CC1(c2ccc(Cl)cc2)OCC(CCl)O1       CI           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/HIV.csv\")\n",
    "df = df.iloc[:2000,:]\n",
    "print(f\"size: {df.shape[0]}\")\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642499d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/HIV_smiles.npy\", np.array(df.smiles), fmt='%s')\n",
    "np.savetxt(\"../../data/HIV_labels.npy\", np.array(df.HIV_active), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b5a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d10908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(smiles):\n",
    "    ## https://github.com/topazape/LSTM_Chem/blob/master/lstm_chem/utils/smiles_tokenizer2.py\n",
    "\n",
    "    atoms = [\n",
    "        'Al', 'As', 'B', 'Br', 'C', 'Cl', 'F', 'H', 'I', 'K', 'Li', 'N',\n",
    "        'Na', 'O', 'P', 'S', 'Se', 'Si', 'Te'\n",
    "    ]\n",
    "    special = [\n",
    "        '(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5',\n",
    "        '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
    "    ]\n",
    "    padding = ['<eos>', '<sos>', '<pad>']\n",
    "\n",
    "    vocab = sorted(atoms, key=len, reverse=True) + special + padding\n",
    "\n",
    "    table_2_chars = list(filter(lambda x: len(x) == 2, vocab))\n",
    "    table_1_chars = list(filter(lambda x: len(x) == 1, vocab))\n",
    "\n",
    "    smiles = smiles + ' '\n",
    "    N = len(smiles)\n",
    "    token = []\n",
    "    i = 0\n",
    "    while (i < N):\n",
    "        c1 = smiles[i]\n",
    "        c2 = smiles[i:i + 2]\n",
    "\n",
    "        if c2 in table_2_chars:\n",
    "            token.append(c2)\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        if c1 in table_1_chars:\n",
    "            token.append(c1)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    for i, tok in enumerate(vocab):\n",
    "        vocab_dict[tok] = i\n",
    "\n",
    "    encoded = [vocab_dict[t] for t in token]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2a3f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3b58f103fb48c09b1ad6be4ba19276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_ids = []\n",
    "    for line in tqdm(df.smiles):\n",
    "        ids = tokenizer(line)\n",
    "        input_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49d604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f5a4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dirs\n",
    "smiles_dir = \"../../data/HIV_smiles.npy\"\n",
    "labels_dir = \"../../data/HIV_labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32927acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label, tokenizer):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.data = np.loadtxt(data, dtype=object)    \n",
    "        self.label = np.loadtxt(label, dtype=float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        input_id = tokenizer(text)\n",
    "        return torch.tensor(input_id, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ff9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = CustomDataset(smiles_dir, labels_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa34668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1600.0], [200.0], [200.0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "[0.8 *len(dataset)], [0.1 *len(dataset)], [0.1 *len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d62ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [1600, 200, 200]) # 80, 10, 10\n",
    "len(train_dataset + test_dataset + val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf5f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID:\n",
      "  tensor([10, 16, 40, 27, 40, 40, 28, 40, 19, 41, 41, 27, 20, 10, 10, 10, 10, 28])\n",
      "Label:\n",
      " tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for smiles, labels in train_dataset:\n",
    "    print(\"Input ID:\\n \" ,smiles)\n",
    "    print(\"Label:\\n\" ,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0225ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list= [], []\n",
    "    for data, label in batch:\n",
    "        ## smiles (copy smiles without...)\n",
    "        processed_text = data.clone().detach() \n",
    "        text_list.append(processed_text)\n",
    "        ## label\n",
    "        label_list.append(label)\n",
    "    label_list = torch.tensor(label_list)\n",
    "        ## padding\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "                       \n",
    "    return padded_text_list, label_list, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03a1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10, 10, 16,  ...,  0,  0,  0],\n",
       "         [16, 23, 10,  ...,  0,  0,  0],\n",
       "         [16, 23, 40,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [18, 23, 17,  ...,  0,  0,  0],\n",
       "         [16, 23, 10,  ...,  0,  0,  0],\n",
       "         [16, 10, 40,  ...,  0,  0,  0]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b449784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=20,shuffle=False, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=20,shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "306151be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 16, 40,  ...,  0,  0,  0],\n",
      "        [10, 10, 19,  ...,  0,  0,  0],\n",
      "        [16, 23, 40,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [10, 10, 15,  ...,  0,  0,  0],\n",
      "        [16, 23, 15,  ...,  0,  0,  0],\n",
      "        [10, 16, 40,  ..., 40, 27, 16]])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(train_dataloader))\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832b557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e84fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4558b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # in_channels; num of channels, we use 1 since its text only\n",
    "        # out_channels; output feature map\n",
    "        # kernel_size; the size of filters i.e. [n-grams size x emb_dim]\n",
    "        # if 3 filters are provided, we get 3 convolution layers\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (filt, embedding_dim)) \n",
    "                                    for filt in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # if we have 3 filter sizes, we get e linear layers\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "    def forward(self, text):\n",
    "        \n",
    "        # add embedding dimension to text\n",
    "        # [batch size, sent len] ==> [batch size, sent len, emb dim]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # add 1 dimensional \n",
    "        # [batch size, sent len, emb dim]==> [batch size, 1, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        # application of Relu activation function\n",
    "        # foward pass of embedd text to convolution layers\n",
    "        # squeeze() to drop superficial 3 dimensional from a tensor\n",
    "        # conved dim ==> [batch size, n_filters, sent len * len(filter_sizes)]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        # max pooling to decrease num of features (sub-sampling)\n",
    "        # squeeze() to drop superficial 2 dimensional from a tensor\n",
    "        # pooled dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # concatenate feature outputs\n",
    "        # cat dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        cat = torch.cat(pooled, dim = 1)\n",
    "        \n",
    "        # dropout on concatenated output filters\n",
    "        # out dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        out = self.dropout(cat)\n",
    "        \n",
    "        # pass filter outputs to linear layer for prediction\n",
    "        # pred dim ==> [batch size, n_filters * len(filter_sizes)]\n",
    "        pred = self.fc(out)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de9a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 60 ## len of vocab size\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1 ## since the output is between 0 and 1, thus 1 dimension\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 0\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b8e9779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(60, 100, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f744f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    # model training mode (gradient computation)\n",
    "    model.train()\n",
    "    # initailiz acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    for idx, (text, label) in enumerate (dataloader):\n",
    "        # reset gradients to zero before each instance\n",
    "        optimizer.zero_grad()\n",
    "        # label predictions (forward papagation)\n",
    "        # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "        predicted_label = model(text).squeeze(1) # or [:,0]\n",
    "        # loss calculation\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        # compute gradients (backward propagation) \n",
    "        # to minimize loss functions with gradient descent\n",
    "        loss.backward()\n",
    "        # update parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## logging\n",
    "        if not idx % 50:\n",
    "            print('| Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f'\n",
    "                  %(epoch +1, num_epochs, idx, len(dataloader), loss))\n",
    "        # compute total accuracy\n",
    "        # if pred label is >=0.5 to probability of true truth accuracy count increases\n",
    "        # summation of the largest value which yields predicted class label\n",
    "        total_acc += ((predicted_label >= 0.5).float() == label).float().sum().item()\n",
    "        # compute total loss after back prop and parameter update\n",
    "        total_loss += loss.item() * label.size(0)\n",
    "    # compare true labels with the predicted labels to compute accuracy\n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "            total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5b6545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    # model evaluation mode (no gradient computation)\n",
    "    model.eval()\n",
    "    # initailize acc, and loss at zero \n",
    "    total_acc, total_loss = 0, 0\n",
    "    # disabling gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for text, label in dataloader:\n",
    "            # label predictions (forward papagation)\n",
    "            # squeeze(1) => drop superficial one dimensional from a tensor\n",
    "            predicted_label = model(text).squeeze(1) # reshape\n",
    "            # loss calculation\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            # compute total accuracy\n",
    "            # if pred label is >=0.5 to probability of true truth accuracy count increases\n",
    "            # summation of the largest value which yields predicted class label\n",
    "            total_acc += ((predicted_label >= 0.5).float() == label).float().sum().item()\n",
    "            # compute total loss after back prop and parameter update\n",
    "            total_loss += loss.item() * label.size(0)\n",
    "            # compare true labels with the predicted labels to compute accuracy\n",
    "        # compare true labels with the predicted labels to compute accuracy\n",
    "        return total_acc/len(dataloader.dataset), \\\n",
    "                total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ef067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binnary classification we use binary cross entropy loss\n",
    "# which provides logits as inputs to loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Adam Optimizer to update parameters based on the computed gradients\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0aea61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c632c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 001/003 | Batch 000/080 | Loss: 0.0006\n",
      "| Epoch: 001/003 | Batch 050/080 | Loss: 0.0098\n",
      "| Epoch: 001/003 | Train Acc: 99.38% | Valid. Acc: 94.50%\n",
      "| Time elapsed: 0.05 min\n",
      "| Epoch: 002/003 | Batch 000/080 | Loss: 0.0033\n",
      "| Epoch: 002/003 | Batch 050/080 | Loss: 0.0370\n",
      "| Epoch: 002/003 | Train Acc: 99.06% | Valid. Acc: 94.50%\n",
      "| Time elapsed: 0.11 min\n",
      "| Epoch: 003/003 | Batch 000/080 | Loss: 0.0253\n",
      "| Epoch: 003/003 | Batch 050/080 | Loss: 0.0193\n",
      "| Epoch: 003/003 | Train Acc: 99.50% | Valid. Acc: 94.50%\n",
      "| Time elapsed: 0.18 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 3\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dataloader)\n",
    "    acc_val , loss_val = evaluate(val_dataloader)\n",
    "    print('| Epoch: %03d/%03d | Train Acc: %.2f%% | Valid. Acc: %.2f%%'\n",
    "          %(epoch + 1, num_epochs, 100 * acc_train, 100 * acc_val))\n",
    "    print(f'| Time elapsed: {(time.time() - start_time) / 60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcfec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d524686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 95.50\n"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dataloader)\n",
    "print('Test Acc: %.2f'\n",
    "      %(100 * acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b38e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in binary classification We use a sigmoid function to squash the input between 0 and 1\n",
    "# unlike in multiclass classification where we use sofmax \n",
    "# or use argmax to get the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66eec46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2_lstm.ipynb\n",
    "\n",
    "sentiment_label = {0: \"low\",\n",
    "                   1: \"medium\"}\n",
    "\n",
    "def predict(text, tokenizer):\n",
    "    # evaluation mode (no gradients)\n",
    "    with torch.no_grad():\n",
    "        # tokenize and index the tokens\n",
    "        processed_text = torch.tensor(tokenizer(text))\n",
    "        # add a batch dimension\n",
    "        processed_text = processed_text.unsqueeze(0).to(device)\n",
    "        # prediction\n",
    "        prediction = model(processed_text)\n",
    "        # reduction real numbers to values between 0 and 1\n",
    "        probability = torch.sigmoid(prediction)\n",
    "        # get the max value of all elements\n",
    "        predicted_probability, predicted_class, = torch.max(probability, dim=1)\n",
    "        # convert tensor holding a single value into an integer\n",
    "        return predicted_class.item(), predicted_probability.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8797b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0 = low\n",
      "Probability: 0.0043982649222016335\n"
     ]
    }
   ],
   "source": [
    "text = \"CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21\"\n",
    "pred_class, pred_proba = predict(text, tokenizer)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "319df42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0 = low\n",
      "Probability: 0.5027364492416382\n"
     ]
    }
   ],
   "source": [
    "text = \"COc1ccc(C)cc1S(=O)(=O)c1c(Cl)cccc1[N+](=O)[O-]\"\n",
    "pred_class, pred_proba = predict(text, tokenizer)\n",
    "\n",
    "print(f'Predicted Class: {pred_class} = {sentiment_label[pred_class]}')\n",
    "print(f'Probability: {pred_proba}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
